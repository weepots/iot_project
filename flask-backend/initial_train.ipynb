{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset import CelebA\n",
    "\n",
    "\n",
    "class gender_classifier:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.USE_CUDA = torch.cuda.is_available()\n",
    "        self.DEVICE = torch.device('cuda' if self.USE_CUDA else 'cpu')\n",
    "        model_ = torchvision.models.resnet101(\n",
    "            weights=torchvision.models.ResNet101_Weights.DEFAULT)\n",
    "        # model_ = torchvision.models.resnet101()\n",
    "        n_inputs = model_.fc.in_features\n",
    "        classifier = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(n_inputs, 1))\n",
    "        ]))\n",
    "        model_.fc = classifier\n",
    "        # model_.load_state_dict(torch.load(\n",
    "        # 'resnet_pretrain_b2.pt', map_location=self.DEVICE), strict=False)\n",
    "        self.model = model_.to(self.DEVICE)\n",
    "        self.transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                              transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "    def train(self, dataloader, model, criterion, optimizer, device):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for idx, (img, target) in tqdm(enumerate(dataloader), total = len(dataloader)):\n",
    "            optimizer.zero_grad()\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            output = F.sigmoid(model(img))\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        return total_loss/len(dataloader)\n",
    "\n",
    "    def train_model(self, epochs=100, learning_rate=0.001):\n",
    "        train_data = CelebA()\n",
    "        # used to reduce the size of the dataset from 200k to 10k\n",
    "        indexes = [x for x in range(10000)]\n",
    "        train_data_subset = Subset(train_data, indices = indexes)\n",
    "        train_loader = DataLoader(train_data_subset, batch_size=32, shuffle=True)\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = self.train(\n",
    "                train_loader, self.model, criterion, optimizer, self.DEVICE)\n",
    "            print(f'Epoch {epoch+1}: Loss {epoch_loss}', flush=True)\n",
    "            torch.save(self.model.state_dict(), 'resnet_pretrain.pt')\n",
    "\n",
    "    def classify_gender(self, image):\n",
    "        self.model.eval()\n",
    "        image = transforms.functional.rotate(img=image, angle=270)\n",
    "        image = transforms.functional.center_crop(\n",
    "            img=image, output_size=[448, 448])\n",
    "        image = transforms.functional.resize(\n",
    "            img=image, size=[224, 224]\n",
    "        )\n",
    "        to_return = image.copy()\n",
    "        image = self.transforms(image).unsqueeze(0)\n",
    "        output_ = self.model(image)\n",
    "        output = F.sigmoid(output_)\n",
    "        return output, to_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('datasets/CelebA/Img/img_align_celeba/000001.jpg', (0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1)), ('datasets/CelebA/Img/img_align_celeba/000002.jpg', (0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1)), ('datasets/CelebA/Img/img_align_celeba/000003.jpg', (0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1)), ('datasets/CelebA/Img/img_align_celeba/000004.jpg', (0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1)), ('datasets/CelebA/Img/img_align_celeba/000005.jpg', (0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1))]\n"
     ]
    }
   ],
   "source": [
    "instance = gender_classifier()\n",
    "c_instance = CelebA()\n",
    "print(c_instance.data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>new_image_attr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_image_00.jpg</td>\n",
       "      <td>(0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_dataset/user_image_1.jpg</td>\n",
       "      <td>(0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      image_name  \\\n",
       "0              user_image_00.jpg   \n",
       "1  user_dataset/user_image_1.jpg   \n",
       "\n",
       "                                      new_image_attr  \n",
       "0  (0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "1  (0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import PIL\n",
    "import pandas as pd\n",
    "import os\n",
    "def add_user_data (gender, image):\n",
    "  user_dataset_location = \"user_dataset\"\n",
    "  user_dataset_csv = os.path.join(user_dataset_location, \"user_dataset_data.csv\")\n",
    "  template = [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1]\n",
    "  gender = \"male\"\n",
    "  if gender == \"male\":\n",
    "    attr_ = 1\n",
    "  else:\n",
    "    attr_= 0\n",
    "  template[20] = attr_\n",
    "  new_image_attr = tuple(template)\n",
    "  \n",
    "  df = pd.read_csv(user_dataset_csv, index_col = False)\n",
    "  last_image_name = df.loc[len(df)-1]['image_name'].split(\"_\")\n",
    "  image_number = int(last_image_name[-1].split(\".\")[0])\n",
    "  new_image_name = f\"user_image_{image_number+1}.jpg\"\n",
    "  new_image_path = os.path.join(user_dataset_location, new_image_name)\n",
    "  image.save(new_image_path)\n",
    "  df.loc[len(df)] = [new_image_path, new_image_attr]\n",
    "  display(df)\n",
    "  df.to_csv(\"./user_dataset/user_dataset_data.csv\", index=False)\n",
    "\n",
    "\n",
    "image = PIL.Image.open(\"000058.jpg\")\n",
    "add_user_data(\"female\", image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('datasets/CelebA/Img/img_align_celeba/202599.jpg', (0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1))\n"
     ]
    }
   ],
   "source": [
    "instance = gender_classifier()\n",
    "c_instance = CelebA()\n",
    "print(c_instance.data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('user_dataset/user_image_1.jpg', (0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/UG/rng037/cz4042/dataset.py:58: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  img_name = df.loc[row][0]\n",
      "/home/UG/rng037/cz4042/dataset.py:61: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  img_attr = literal_eval(df.loc[row][1])\n"
     ]
    }
   ],
   "source": [
    "c_instance.add_user_dataset()\n",
    "print(c_instance.data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
